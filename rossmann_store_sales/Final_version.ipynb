{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.0 IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy    as np\n",
    "import pandas   as pd\n",
    "import inflection\n",
    "import datetime as dt\n",
    "import seaborn  as sns\n",
    "import xgboost  as xgb\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "from scipy                import stats as ss\n",
    "from boruta               import BorutaPy\n",
    "from tabulate             import tabulate\n",
    "from matplotlib           import pyplot as plt\n",
    "from IPython.display      import Image\n",
    "from IPython.display import HTML\n",
    "\n",
    "from sklearn.metrics       import mean_absolute_error, mean_absolute_percentage_error, mean_squared_error\n",
    "\n",
    "from sklearn.ensemble      import RandomForestRegressor\n",
    "from sklearn.linear_model  import LinearRegression, Lasso\n",
    "from sklearn.preprocessing import RobustScaler, MinMaxScaler, LabelEncoder\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1 Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(x_training, kfold, model_name, model, verbose=False):\n",
    "    # cria listas vazias para armazenar as métricas de erro de cada dobra\n",
    "    mae_list = []\n",
    "    mape_list = []\n",
    "    rmse_list = []\n",
    "\n",
    "    # itera através das dobras, começando pela última e indo até a primeira\n",
    "    for k in reversed(range(1, kfold + 1)):\n",
    "        # imprime o número da dobra atual, se verbose for True\n",
    "        if verbose:\n",
    "            print('\\nKFold Number: {}'.format(k))\n",
    "\n",
    "        # define as datas de início e término para a validação\n",
    "        validation_start_date = x_training['date'].max() - dt.timedelta(days=k*6*7)\n",
    "        validation_end_date = x_training['date'].max() - dt.timedelta(days=(k-1)*6*7)\n",
    "\n",
    "        # filtra o conjunto de dados para obter o conjunto de treinamento e validação\n",
    "        training = x_training[x_training['date'] < validation_start_date]\n",
    "        validation = x_training[(x_training['date'] >= validation_start_date) & (x_training['date'] <= validation_end_date)]\n",
    "\n",
    "        # separa as variáveis de entrada e saída para o conjunto de treinamento e validação\n",
    "        xtraining = training.drop(['date', 'sales'], axis=1)\n",
    "        ytraining = training['sales']\n",
    "        xvalidation = validation.drop(['date', 'sales'], axis=1)\n",
    "        yvalidation = validation['sales']\n",
    "\n",
    "        # treina o modelo com o conjunto de treinamento\n",
    "        m = model.fit(xtraining, ytraining)\n",
    "\n",
    "        # faz a previsão com o conjunto de validação\n",
    "        yhat = m.predict(xvalidation)\n",
    "\n",
    "        # calcula as métricas de erro e armazena em listas\n",
    "        m_result = ml_error(model_name, np.expm1(yvalidation), np.expm1(yhat))\n",
    "        mae_list.append(m_result['MAE'])\n",
    "        mape_list.append(m_result['MAPE'])\n",
    "        rmse_list.append(m_result['RMSE'])\n",
    "\n",
    "    # calcula a média e o desvio padrão das métricas de erro e retorna em um DataFrame\n",
    "    return pd.DataFrame({\n",
    "        'Model Name': model_name,\n",
    "        'MAE CV': np.round(np.mean(mae_list), 2).astype(str) + ' +/- ' + np.round(np.std(mae_list), 2).astype(str),\n",
    "        'MAPE CV': np.round(np.mean(mape_list), 2).astype(str) + ' +/- ' + np.round(np.std(mape_list), 2).astype(str),\n",
    "        'RMSE CV': np.round(np.mean(rmse_list), 2).astype(str) + ' +/- ' + np.round(np.std(rmse_list), 2).astype(str)\n",
    "    }, index=[0])\n",
    "\n",
    "def mean_percentage_error(y, yhat):\n",
    "    # Calcula o erro percentual médio entre os valores reais e as previsões do modelo.\n",
    "    return np.mean((y - yhat) / y) * 100\n",
    "\n",
    "\n",
    "\n",
    "# esta função calcula os erros de desempenho (MAE, MAPE e RMSE) entre os valores reais e previstos\n",
    "def ml_error( model_name, y, yhat ):\n",
    "    # cálculo do erro absoluto médio (MAE)\n",
    "    mae = mean_absolute_error( y, yhat )\n",
    "    # cálculo do erro percentual absoluto médio (MAPE)\n",
    "    mape = mean_absolute_percentage_error( y, yhat )\n",
    "    # cálculo da raiz do erro quadrático médio (RMSE)\n",
    "    rmse = np.sqrt( mean_squared_error( y, yhat ) )\n",
    "    # armazenando os resultados em um dataframe pandas\n",
    "    return pd.DataFrame( { 'Model Name': model_name,\n",
    "                           'MAE': mae,\n",
    "                           'MAPE': mape,\n",
    "                           'RMSE': rmse }, index=[0] )\n",
    "\n",
    "\n",
    "# calcula a medida de associação entre duas variáveis categóricas usando o coeficiente de Cramer's V\n",
    "def cramer_v( x, y ):\n",
    "    # cria uma tabela de contingência a partir das duas variáveis categóricas x e y, e transforma em uma matriz NumPy\n",
    "    cm = pd.crosstab( x, y ).to_numpy()\n",
    "    n = cm.sum()\n",
    "    r, k = cm.shape\n",
    "\n",
    "    # calcula o qui-quadrado para a tabela de contingência cm, e retorna o valor qui-quadrado.\n",
    "    chi2 = ss.chi2_contingency( cm )[0]\n",
    "    \n",
    "    # calcula o valor corrigido de qui-quadrado, que leva em conta o tamanho da tabela de contingência.\n",
    "    chi2corr = max( 0, chi2 - (k-1)*(r-1)/(n-1) )\n",
    "\n",
    "    # calcula o número de colunas corrigido.\n",
    "    kcorr = k - (k-1)**2/(n-1)\n",
    "\n",
    "    # calcula o número de linhas corrigido.\n",
    "    rcorr = r - (r-1)**2/(n-1)\n",
    "\n",
    "    # 'min(kcorr-1, rcorr-1)': calcula o menor valor entre o número de colunas corrigido menos 1 e o número de linhas corrigido menos 1.\n",
    "    # 'np.sqrt((chi2corr/n) / (min(kcorr-1, rcorr-1)))': calcula o coeficiente de Cramer's V como a raiz quadrada da razão entre o valor corrigido de qui-quadrado e o menor valor entre o número de colunas e o número de linhas corrigido menos 1.\n",
    "    return np.sqrt( (chi2corr/n) / ( min( kcorr-1, rcorr-1 ) ) )\n",
    "\n",
    "\n",
    "def jupyter_settings():\n",
    "    %matplotlib inline\n",
    "    %pylab inline\n",
    "    \n",
    "    plt.style.use( 'bmh' )\n",
    "    plt.rcParams['figure.figsize'] = [25, 12]\n",
    "    plt.rcParams['font.size'] = 24\n",
    "    \n",
    "    display(HTML( '<style>.container { width:100% !important; }</style>'))\n",
    "    pd.options.display.max_columns = None\n",
    "    pd.options.display.max_rows = None\n",
    "    pd.set_option( 'display.expand_frame_repr', False )\n",
    "    \n",
    "    sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%pylab is deprecated, use %matplotlib inline and import the required libraries.\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\raquel\\anaconda3\\envs\\ds_em_producao\\lib\\site-packages\\IPython\\core\\magics\\pylab.py:162: UserWarning: pylab import has clobbered these variables: ['random']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  warn(\"pylab import has clobbered these variables: %s\"  % clobbered +\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "jupyter_settings()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.2 Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Lê o arquivo todo de um vez\n",
    "df_sales_raw = pd.read_csv('data/train.csv', low_memory=False)\n",
    "\n",
    "df_store_raw = pd.read_csv('data/store.csv', low_memory=False)\n",
    "\n",
    "# merge, 1º arquivo de referencia, 2º arquivo anexado a referencia, left, \n",
    "# coluna que é igual nos dois datasets e serve como chave para eu fazer o merge\n",
    "df_raw = pd.merge(df_sales_raw, df_store_raw, how='left', on='Store')\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0 DESCRIÇÃO DOS DADOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df1 = df_raw.copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Rename Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_old = ['Store', 'DayOfWeek', 'Date', 'Sales', 'Customers', 'Open', 'Promo',\n",
    "            'StateHoliday', 'SchoolHoliday', 'StoreType', 'Assortment',\n",
    "            'CompetitionDistance', 'CompetitionOpenSinceMonth',\n",
    "            'CompetitionOpenSinceYear', 'Promo2', 'Promo2SinceWeek',\n",
    "            'Promo2SinceYear', 'PromoInterval']\n",
    "\n",
    "snakecase = lambda x: inflection.underscore(x)\n",
    "\n",
    "cols_new = list(map (snakecase, cols_old))\n",
    "\n",
    "#rename\n",
    "df1.columns = cols_new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mundando o tipo da coluna 'date'\n",
    "df1['date'] = pd.to_datetime(df1['date'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Fillout NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# competition_distance \n",
    "# \n",
    "# tornar os NA em uma distancia maior que a maior distancia já encontrada no dataset\n",
    "df1['competition_distance'] = df1['competition_distance'].apply(lambda x: 200000.0 if math.isnan(x) else x)         \n",
    "\n",
    "# competition_open_since_month    \n",
    "df1['competition_open_since_month'] = df1.apply(lambda x: x['date'].month if math.isnan(x['competition_open_since_month']) else x['competition_open_since_month'], axis=1)\n",
    "\n",
    "# competition_open_since_year     \n",
    "df1['competition_open_since_year'] = df1.apply(lambda x: x['date'].year if math.isnan(x['competition_open_since_year']) else x['competition_open_since_year'], axis=1)\n",
    "\n",
    "# promo2_since_week               \n",
    "df1['promo2_since_week'] = df1.apply(lambda x: x['date'].week if math.isnan(x['promo2_since_week']) else x['promo2_since_week'], axis=1)\n",
    "\n",
    "# promo2_since_year               \n",
    "df1['promo2_since_year'] = df1.apply(lambda x: x['date'].year if math.isnan(x['promo2_since_year']) else x['promo2_since_year'], axis=1)\n",
    "\n",
    "\n",
    "# promo_interval\n",
    "\n",
    "#dicionario que associa o mês ao nome do mês\n",
    "month_map = {1: 'Jan', 2: 'Fev', 3: 'Mar', 4: 'Apr', 5: 'May', 6: 'Jun', 7: 'Jul', 8: 'Aug', 9: 'Sep', 10: 'Ouc', 11: 'Nov', 12: 'Dec',}\n",
    " \n",
    "#preenche com NA para não ter que comparar usando ISNAN, mas poderia ter feito usando ISNAN\n",
    "df1['promo_interval'].fillna(0, inplace=True)\n",
    "\n",
    "\n",
    "#extrai o mês da coluna data e aplica o dicionario para fazer a tradução para o nome do mes\n",
    "df1['month_map'] = df1['date'].dt.month.map(month_map)\n",
    "\n",
    "# aplica a condicional para saber se o month_map está dentro daquele intervalo para saber se a loja está ou não em promoção\n",
    "df1['is_promo'] = df1[['promo_interval', 'month_map']].apply(lambda x: 0 if x['promo_interval'] == 0 else 1 if x['month_map'] in x['promo_interval'].split(',') else 0, axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Change Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['competition_open_since_month'] = df1['competition_open_since_month'].astype(int)\n",
    "df1['competition_open_since_year'] = df1['competition_open_since_year'].astype(int)\n",
    "df1['promo2_since_week'] = df1['promo2_since_week'].astype(int)\n",
    "df1['promo2_since_year'] = df1['promo2_since_year'].astype(int)\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7 Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_attributes = df1.select_dtypes(include=['int64', 'float64', 'int32'])\n",
    "cat_attributes = df1.select_dtypes(exclude=['int64', 'float64', 'int32', 'datetime64[ns]'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.0 FEATURE ENGINEERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df1.copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# year\n",
    "df2['year'] = df2['date'].dt.year\n",
    "\n",
    "# month\n",
    "df2['month'] = df2['date'].dt.month\n",
    "\n",
    "# day\n",
    "df2['day'] = df2['date'].dt.day\n",
    "\n",
    "# week of year\n",
    "df2['week_of_year'] = df2['date'].dt.isocalendar().week\n",
    "\n",
    "# year week\n",
    "df2['year_week'] = df2['date'].dt.strftime('%Y-%W')\n",
    "\n",
    "# competition since\n",
    "df2['competition_since'] = df2.apply(lambda x: dt.datetime(year=x['competition_open_since_year'], month=x['competition_open_since_month'], day=1), axis=1 )\n",
    "df2['competition_time_month'] = ((df2['date'] - df2['competition_since'] )/30).apply(lambda x: x.days).astype(int)\n",
    "\n",
    "# promo since\n",
    "df2['promo_since'] = df2['promo2_since_year'].astype(str) + '-' + df2['promo2_since_week'].astype(str)\n",
    "df2['promo_since'] = df2['promo_since'].apply(lambda x: dt.datetime.strptime(x + '-1', '%Y-%W-%w') - dt.timedelta(days=7))\n",
    "df2['promo_time_week'] = ((df2['date'] - df2['promo_since'])/7).apply(lambda x: x.days).astype(int)\n",
    "\n",
    "# assortment\n",
    "df2['assortment'] = df2['assortment'].apply(lambda x: 'basic' if x == 'a' else 'extra' if x == 'b' else 'extended')\n",
    "\n",
    "# state holiday\n",
    "df2['state_holiday'] = df2['state_holiday'].apply(lambda x: 'public_holiday' if x == 'a' else 'easter_holiday' if x == 'b' else 'christmas' if x == 'c' else 'regular_day')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.0 FILTRAGEM DE VARIÁVEIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df2.copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Filtragem das Linhas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df3[(df3['open'] != 0) & (df3['sales'] > 0)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Selecao das Colunas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_drop = ['customers', 'open', 'promo_interval', 'month_map']\n",
    "df3 = df3.drop(cols_drop, axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.0 ANALISE EXPLORATORIA DOS DADOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = df3.copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.0 DATA PREPARATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = df4.copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Rescaling\n",
    "\n",
    "###### o rescaling é utilizado para ajustar a escala dos dados para um intervalo específico, como 0 e 1 ou -1 e 1. Isso pode ser útil para modelos que utilizam funções de ativação específicas, como redes neurais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Técnica de normalização robusta a outliers, que escalona os dados de forma que a mediana seja igual a 0 e a distribuição interquartil (IQR) seja igual a 1.\n",
    "rs = RobustScaler()\n",
    "\n",
    "# Método de rescaling que transforma os dados de forma que eles fiquem no intervalo de 0 a 1.\n",
    "mms = MinMaxScaler()\n",
    "\n",
    "# Após aplicar os escalonamentos, o código sobrescreve as variáveis originais no dataframe df5 com os valores escalonados.\n",
    "\n",
    "# Escalonamento da variável competition distance com RobustScaler\n",
    "df5['competition_distance'] = rs.fit_transform(df5[['competition_distance']].values)\n",
    "# Salva o objeto rs (RobustScaler) em um arquivo pickle para uso futuro\n",
    "pickle.dump(rs, open('parameter/competition_distance_scaler.pkl', 'wb'))\n",
    "\n",
    "# Escalonamento da variável competition time month com RobustScaler\n",
    "df5['competition_time_month'] = rs.fit_transform(df5[['competition_time_month']].values)\n",
    "# Salva o objeto rs (RobustScaler) em um arquivo pickle para uso futuro\n",
    "pickle.dump(rs, open('parameter/competition_time_month_scaler.pkl', 'wb'))\n",
    "\n",
    "# Escalonamento da variável promo time week com MinMaxScaler\n",
    "df5['promo_time_week'] = mms.fit_transform(df5[['promo_time_week']].values)\n",
    "# Salva o objeto mms (MinMaxScaler) em um arquivo pickle para uso futuro\n",
    "pickle.dump(mms, open('parameter/promo_time_week_scaler.pkl', 'wb'))\n",
    "\n",
    "# Escalonamento da variável year com MinMaxScaler\n",
    "df5['year'] = mms.fit_transform(df5[['year']].values)\n",
    "# Salva o objeto mms (MinMaxScaler) em um arquivo pickle para uso futuro\n",
    "pickle.dump(mms, open('parameter/year_scaler.pkl', 'wb'))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Transformação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state_holiday - One Hot Encoding\n",
    "df5 = pd.get_dummies(df5, prefix=['state_holiday'], columns=['state_holiday'])\n",
    "\n",
    "# store_type - Label Encoding\n",
    "le = LabelEncoder()\n",
    "df5['store_type'] = le.fit_transform(df5['store_type'])\n",
    "#pickle.dump(le, open('parameter/store_type_scaler.pkl', 'wb'))\n",
    "\n",
    "# assortment - Ordinal Encoding\n",
    "assortment_dict = {'basic': 1, 'extra': 2, 'extended': 3}\n",
    "df5['assortment'] = df5['assortment'].map(assortment_dict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.2 Response Variable Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5['sales'] = np.log1p(df5['sales'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.3 Nature Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adiciona colunas com o seno e cosseno dos dias da semana, meses, dias e semana do ano\n",
    "# Isso é feito por se tratar de dados cíclicos.\n",
    "\n",
    "# day of week\n",
    "df5['day_of_week_sin'] = df5['day_of_week'].apply(lambda x: np.sin(x * (2. * np.pi/7)))\n",
    "df5['day_of_week_cos'] = df5['day_of_week'].apply(lambda x: np.cos(x * (2. * np.pi/7)))\n",
    "\n",
    "# month\n",
    "df5['month_sin'] = df5['month'].apply(lambda x: np.sin(x * ( 2. * np.pi/12)))\n",
    "df5['month_cos'] = df5['month'].apply(lambda x: np.cos(x * ( 2. * np.pi/12)))\n",
    "\n",
    "# day\n",
    "df5['day_sin'] = df5['day'].apply(lambda x: np.sin(x * (2. * np.pi/30)))\n",
    "df5['day_cos'] = df5['day'].apply(lambda x: np.cos(x * (2. * np.pi/30)))\n",
    "\n",
    "# week of year\n",
    "df5['week_of_year_sin'] = df5['week_of_year'].apply(lambda x: np.sin(x * (2. * np.pi/52)))\n",
    "df5['week_of_year_cos'] = df5['week_of_year'].apply(lambda x: np.cos(x * (2. * np.pi/52))) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.0 FEATURE SELECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df6 = df5.copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Split dataframe into training and test datase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lista das colunas que serão removidas\n",
    "cols_drop = ['week_of_year', 'day', 'month', 'day_of_week', 'promo_since', 'competition_since', 'year_week' ]\n",
    "\n",
    "# remove as colunas especificadas\n",
    "df6 = df6.drop(cols_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Min Date: 2013-01-01 00:00:00\n",
      "Training Max Date: 2015-06-18 00:00:00\n",
      "\n",
      "Test Min Date: 2015-06-19 00:00:00\n",
      "Test Max Date: 2015-07-31 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# seleciona as linhas do DataFrame que possuem datas anteriores a '2015-06-19'\n",
    "X_train = df6[df6['date'] < '2015-06-19']\n",
    "\n",
    "# seleciona a coluna 'sales' do DataFrame 'X_train' e armazena em 'y_train'\n",
    "y_train = X_train['sales']\n",
    "\n",
    "# seleciona as linhas do DataFrame que possuem datas a partir de '2015-06-19'\n",
    "X_test = df6[df6['date'] >= '2015-06-19']\n",
    "\n",
    "# seleciona a coluna 'sales' do DataFrame 'X_test' e armazena em 'y_test'\n",
    "y_test = X_test['sales']\n",
    "\n",
    "# exibe a data mínima e máxima do conjunto de treinamento\n",
    "print('Training Min Date: {}'.format( X_train['date'].min()))\n",
    "print('Training Max Date: {}'.format( X_train['date'].max()))\n",
    "\n",
    "# exibe a data mínima e máxima do conjunto de teste\n",
    "print('\\nTest Min Date: {}'.format( X_test['date'].min()))\n",
    "print('Test Max Date: {}'.format( X_test['date'].max()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Manual Feature Selection - BORUTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista com as colunas selecionadas pelo algoritmo Boruta\n",
    "cols_selected_boruta = [\n",
    "    'store',\n",
    "    'promo',\n",
    "    'store_type',\n",
    "    'assortment',\n",
    "    'competition_distance',\n",
    "    'competition_open_since_month',\n",
    "    'competition_open_since_year',\n",
    "    'promo2',\n",
    "    'promo2_since_week',\n",
    "    'promo2_since_year',\n",
    "    'competition_time_month',\n",
    "    'promo_time_week',\n",
    "    'day_of_week_sin',\n",
    "    'day_of_week_cos',\n",
    "    'month_sin',\n",
    "    'month_cos',\n",
    "    'day_sin',\n",
    "    'day_cos',\n",
    "    'week_of_year_sin',\n",
    "    'week_of_year_cos']\n",
    "\n",
    "# Colunas que precisam ser mantidas no conjunto de dados\n",
    "feat_to_add = ['date', 'sales']\n",
    "\n",
    "# Lista completa de colunas selecionadas\n",
    "cols_selected_boruta_full = cols_selected_boruta.copy()\n",
    "cols_selected_boruta_full.extend(feat_to_add)\n",
    "\n",
    "\n",
    "# O resultado final é uma lista cols_selected_boruta_full com os nomes das colunas selecionadas pelo Boruta e as colunas adicionais 'date' e 'sales'."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.0 MACHINE LEARNING MODELLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecionando as colunas de treinamento e teste que foram selecionadas pelo algoritmo Boruta\n",
    "x_train = X_train[cols_selected_boruta]\n",
    "x_test = X_test[cols_selected_boruta]\n",
    "\n",
    "# Time Series Data Preparation\n",
    "# Selecionando todas as colunas relevantes, incluindo data e vendas, para treinamento\n",
    "x_training = X_train[cols_selected_boruta_full]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2 Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define um dicionário com valores fixos para os hiperparâmetros do modelo\n",
    "param_tuned = {\n",
    "    'n_estimators': 3000,\n",
    "    'eta': 0.03,\n",
    "    'max_depth': 5,\n",
    "    'subsample': 0.7,\n",
    "    'colsample_bytree': 0.7,\n",
    "    'min_child_weight': 3\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model Name</th>\n",
       "      <th>MAE</th>\n",
       "      <th>MAPE</th>\n",
       "      <th>RMSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XGBoost Regressor</td>\n",
       "      <td>770.209978</td>\n",
       "      <td>0.115623</td>\n",
       "      <td>1108.062869</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Model Name         MAE      MAPE         RMSE\n",
       "0  XGBoost Regressor  770.209978  0.115623  1108.062869"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cria um modelo XGBoostRegressor com os valores especificados em 'param_tuned' e ajusta aos dados de treinamento\n",
    "model_xgb_tuned = xgb.XGBRegressor(\n",
    "    objective='reg:squarederror',\n",
    "    n_estimators=param_tuned['n_estimators'],\n",
    "    eta=param_tuned['eta'],\n",
    "    max_depth=param_tuned['max_depth'],\n",
    "    subsample=param_tuned['subsample'],\n",
    "    colsample_bytree=param_tuned['colsample_bytree'],\n",
    "    min_child_weight=param_tuned['min_child_weight']\n",
    "    ).fit(x_train, y_train)\n",
    "\n",
    "# Faz previsões no conjunto de teste usando o modelo ajustado\n",
    "yhat_xgb_tuned = model_xgb_tuned.predict(x_test)\n",
    "\n",
    "# Avalia o desempenho do modelo usando a função 'ml_error'\n",
    "xgb_result_tuned = ml_error(\n",
    "    'XGBoost Regressor',\n",
    "    np.expm1(y_test), # desfaz a transformação logarítmica aplicada nos dados de saída (y)\n",
    "    np.expm1(yhat_xgb_tuned) # desfaz a transformação logarítmica aplicada nas previsões (yhat)\n",
    ")\n",
    "\n",
    "xgb_result_tuned\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9.0 TRADUCAO E INTERPRETACAO DO ERRO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\raquel\\AppData\\Local\\Temp\\ipykernel_11748\\2332760266.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df9['sales'] = np.expm1(df9['sales'])\n",
      "C:\\Users\\raquel\\AppData\\Local\\Temp\\ipykernel_11748\\2332760266.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df9['predictions'] = np.expm1(yhat_xgb_tuned)\n"
     ]
    }
   ],
   "source": [
    "# Selecionando as colunas relevantes do conjunto de dados de teste e armazenando em df9\n",
    "df9 = X_test[cols_selected_boruta_full]\n",
    "\n",
    "# Convertendo as previsões e os valores reais para a escala original de vendas usando np.expm1()\n",
    "# A coluna \"sales\" contém os valores reais e \"predictions\" contém as previsões do modelo\n",
    "df9['sales'] = np.expm1(df9['sales'])\n",
    "df9['predictions'] = np.expm1(yhat_xgb_tuned)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1 Business Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculando a soma das previsões por loja e armazenando em df91\n",
    "df91 = df9[['store', 'predictions']].groupby('store').sum().reset_index()\n",
    "\n",
    "# Calculando o erro absoluto médio (MAE) por loja e armazenando em df9_aux1\n",
    "df9_aux1 = df9[['store', 'sales', 'predictions']].groupby('store').apply(\n",
    "    lambda x: mean_absolute_error(x['sales'], x['predictions'])\n",
    ").reset_index().rename(columns={0: 'MAE'})\n",
    "\n",
    "# Calculando o erro percentual absoluto médio (MAPE) por loja e armazenando em df9_aux2\n",
    "df9_aux2 = df9[['store', 'sales', 'predictions']].groupby('store').apply(\n",
    "    lambda x: mean_absolute_percentage_error(x['sales'], x['predictions'])\n",
    ").reset_index().rename(columns={0: 'MAPE'})\n",
    "\n",
    "# Juntando os DataFrames df9_aux1 e df9_aux2 em df9_aux3, usando a coluna 'store' como chave\n",
    "df9_aux3 = pd.merge(df9_aux1, df9_aux2, how='inner', on='store')\n",
    "\n",
    "# Juntando os DataFrames df91 e df9_aux3 em df92, usando a coluna 'store' como chave\n",
    "df92 = pd.merge(df91, df9_aux3, how='inner', on='store')\n",
    "\n",
    "# Calculando os cenários 'pior caso' e 'melhor caso' usando o MAE, e adicionando como colunas em df92\n",
    "df92['worst_scenario'] = df92['predictions'] - df92['MAE']\n",
    "df92['best_scenario'] = df92['predictions'] + df92['MAE']\n",
    "\n",
    "# Reordenando as colunas de df92\n",
    "df92 = df92[['store', 'predictions', 'worst_scenario', 'best_scenario', 'MAE', 'MAPE']]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10.0 DEPLOY MODEL TO PRODUCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Este código salva o modelo model_xgb_tuned que foi treinado em um\\n arquivo com o nome model_rossmann.pkl. O caminho completo para este arquivo \\n é especificado como um argumento para a função open(). O prefixo r antes das \\n aspas duplas indica que esta é uma \"string raw\" que desativa o processamento \\n de caracteres de escape, para que as barras invertidas no caminho do arquivo \\n não sejam interpretadas como caracteres de escape. A opção wb indica que o \\n arquivo deve ser aberto para escrita em modo binário, o que é necessário para \\n salvar objetos Python em formato binário usando a função pickle.dump().'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save Trained Model\n",
    "pickle.dump(model_xgb_tuned, open(r'C:\\Users\\raquel\\Documents\\Comunidade DS\\repos\\05-DS-emProducao\\rossmann_store_sales\\model\\model_rossmann.pkl', 'wb'))\n",
    "\n",
    "\n",
    "'''Este código salva o modelo model_xgb_tuned que foi treinado em um\n",
    " arquivo com o nome model_rossmann.pkl. O caminho completo para este arquivo \n",
    " é especificado como um argumento para a função open(). O prefixo r antes das \n",
    " aspas duplas indica que esta é uma \"string raw\" que desativa o processamento \n",
    " de caracteres de escape, para que as barras invertidas no caminho do arquivo \n",
    " não sejam interpretadas como caracteres de escape. A opção wb indica que o \n",
    " arquivo deve ser aberto para escrita em modo binário, o que é necessário para \n",
    " salvar objetos Python em formato binário usando a função pickle.dump().'''\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.1 Rossmann Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import inflection\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import datetime\n",
    "\n",
    "\n",
    "class Rossmann(object):\n",
    "    def __init__(self):\n",
    "        # Diretório do projeto\n",
    "        self.home_path = 'C:\\\\Users\\\\raquel\\\\Documents\\\\Comunidade DS\\\\repos\\\\05-DS-emProducao\\\\rossmann_store_sales\\\\'\n",
    "       \n",
    "        # Carrega os modelos pré-treinados para normalização\n",
    "        self.competition_distance_scaler = pickle.load(open(self.home_path + 'parameter\\\\competition_distance_scaler.pkl', 'rb'))\n",
    "        self.competition_time_month_scaler = pickle.load(open(self.home_path + 'parameter\\\\competition_time_month_scaler.pkl', 'rb'))\n",
    "        self.promo_time_week_scaler = pickle.load(open(self.home_path + 'parameter\\\\promo_time_week_scaler.pkl', 'rb'))\n",
    "        self.year_scaler = pickle.load(open(self.home_path + 'parameter\\\\year_scaler.pkl', 'rb'))\n",
    "        self.store_type_scaler = pickle.load(open(self.home_path + 'parameter\\\\store_type_scaler.pkl', 'rb'))\n",
    "\n",
    "    def data_cleaning(self, df1):\n",
    "        ## 1.1. Renomeando Colunas\n",
    "        # Lista de colunas antigas\n",
    "        cols_old = ['Store', 'DayOfWeek', 'Date', 'Open', 'Promo', 'StateHoliday', 'SchoolHoliday',\n",
    "                    'StoreType', 'Assortment', 'CompetitionDistance', 'CompetitionOpenSinceMonth',\n",
    "                    'CompetitionOpenSinceYear', 'Promo2', 'Promo2SinceWeek', 'Promo2SinceYear', 'PromoInterval']\n",
    "        \n",
    "        # Transformando o nome das colunas em snake_case\n",
    "        snakecase = lambda x: inflection.underscore(x)\n",
    "        cols_new = list(map(snakecase, cols_old))\n",
    "        \n",
    "        # renomeando as colunas do dataframe\n",
    "        df1.columns = cols_new\n",
    "\n",
    "        ## 1.3. Tipos de Dados\n",
    "        # Convertendo a coluna \"date\" para o tipo datetime\n",
    "        df1['date'] = pd.to_datetime(df1['date'])\n",
    "\n",
    "        ## 1.5. Preenchimento de Valores Faltantes\n",
    "        # competition_distance\n",
    "        # Substituindo valores faltantes por 200000.0\n",
    "        df1['competition_distance'] = df1['competition_distance'].apply(lambda x: 200000.0 if math.isnan(x) else x)\n",
    "        \n",
    "        # competition_open_since_month\n",
    "        # Substituindo valores faltantes pelo mês da coluna \"date\"\n",
    "        df1['competition_open_since_month'] = df1.apply(lambda x: x['date'].month if math.isnan(x['competition_open_since_month']) else x['competition_open_since_month'], axis=1)\n",
    "        \n",
    "        # competition_open_since_year\n",
    "        # Substituindo valores faltantes pelo ano da coluna \"date\"\n",
    "        df1['competition_open_since_year'] = df1.apply(lambda x: x['date'].year if math.isnan(x['competition_open_since_year']) else x['competition_open_since_year'], axis=1)\n",
    "        \n",
    "        # promo2_since_week\n",
    "        # Substituindo valores faltantes pelo número da semana da coluna \"date\"\n",
    "        df1['promo2_since_week'] = df1.apply(lambda x: x['date'].week if math.isnan(x['promo2_since_week']) else x['promo2_since_week'], axis=1)\n",
    "        \n",
    "        # promo2_since_year\n",
    "        # Substitui os valores faltantes da coluna \"promo2_since_year\" pelo ano presente na coluna \"date\" caso seja possível\n",
    "        df1['promo2_since_year'] = df1.apply(lambda x: x['date'].year if math.isnan(x['promo2_since_year']) else x['promo2_since_year'], axis=1)\n",
    "\n",
    "        # promo_interval\n",
    "        # Mapeamento dos meses do ano para as siglas correspondentes\n",
    "        month_map = {1: 'Jan', 2: 'Fev', 3: 'Mar', 4: 'Apr', 5: 'May', 6: 'Jun',\n",
    "                    7: 'Jul', 8: 'Aug', 9: 'Sep', 10: 'Oct', 11: 'Nov', 12: 'Dec'}\n",
    "\n",
    "        # Substitui os valores faltantes na coluna \"promo_interval\" por 0\n",
    "        df1['promo_interval'].fillna(0, inplace=True )\n",
    "\n",
    "        # Cria uma nova coluna \"month_map\" que mapeia o número do mês para as siglas correspondentes\n",
    "        df1['month_map'] = df1['date'].dt.month.map( month_map )\n",
    "\n",
    "        # Cria uma nova coluna \"is_promo\" que informa se determinada loja esteve em promoção ou não no mês da data presente na linha\n",
    "        df1['is_promo'] = df1[['promo_interval', 'month_map']].apply( lambda x:\n",
    "            0 if x['promo_interval'] == 0 else 1 if x['month_map'] in\n",
    "            x['promo_interval'].split( ',' ) else 0, axis=1 )\n",
    "            \n",
    "\n",
    "        ## 1.6. Change Data Types\n",
    "        # competiton\n",
    "        # Converte a coluna \"competition_open_since_month\" para tipo inteiro\n",
    "        df1['competition_open_since_month'] = df1['competition_open_since_month'].astype( int )\n",
    "\n",
    "        # Converte a coluna \"competition_open_since_year\" para tipo inteiro\n",
    "        df1['competition_open_since_year'] = df1['competition_open_since_year'].astype( int )\n",
    "\n",
    "        # promo2\n",
    "        # Converte a coluna \"promo2_since_week\" para tipo inteiro\n",
    "        df1['promo2_since_week'] = df1['promo2_since_week'].astype( int )\n",
    "\n",
    "        # Converte a coluna \"promo2_since_year\" para tipo inteiro\n",
    "        df1['promo2_since_year'] = df1['promo2_since_year'].astype( int )\n",
    "\n",
    "        # Retorna o dataframe com as alterações realizadas\n",
    "        return df1\n",
    "    \n",
    "\n",
    "    def feature_engineering(self, df2):\n",
    "        # year\n",
    "        df2['year'] = df2['date'].dt.year\n",
    "        \n",
    "        # month\n",
    "        df2['month'] = df2['date'].dt.month\n",
    "        \n",
    "        # day\n",
    "        df2['day'] = df2['date'].dt.day\n",
    "        \n",
    "        # week of year\n",
    "        df2['week_of_year'] = df2['date'].dt.weekofyear\n",
    "        \n",
    "        # year week\n",
    "        df2['year_week'] = df2['date'].dt.strftime('%Y-%W')\n",
    "        \n",
    "        # competition since\n",
    "        df2['competition_since'] = df2.apply(lambda x: datetime.datetime(\n",
    "            year=x['competition_open_since_year'],\n",
    "            month=x['competition_open_since_month'],\n",
    "            day=1\n",
    "        ), axis=1)\n",
    "        df2['competition_time_month'] = ((df2['date'] - df2['competition_since']) / 30).apply(lambda x: x.days).astype(int)\n",
    "        \n",
    "        # promo since\n",
    "        df2['promo_since'] = df2['promo2_since_year'].astype(str) + '-' + df2['promo2_since_week'].astype(str)\n",
    "        df2['promo_since'] = df2['promo_since'].apply(lambda x: datetime.datetime.strptime(x + '-1', '%Y-%W-%w') - datetime.timedelta(days=7))\n",
    "        df2['promo_time_week'] = ((df2['date'] - df2['promo_since']) / 7).apply(lambda x: x.days).astype(int)\n",
    "        \n",
    "        # assortment\n",
    "        df2['assortment'] = df2['assortment'].apply(lambda x: 'basic' if x == 'a' else 'extra' if x == 'b' else 'extended')\n",
    "        \n",
    "        # state holiday\n",
    "        df2['state_holiday'] = df2['state_holiday'].apply(lambda x: 'public_holiday' if x == 'a' else 'easter_holiday' if x == 'b' else 'christmas' if x == 'c' else 'regular_day')\n",
    "        \n",
    "        # 3.0 - FILTRAGEM DE VARIÁVEIS\n",
    "        ## 3.1. Filtragem das Linhas\n",
    "        df2 = df2[df2['open'] != 0]\n",
    "        \n",
    "        ## 3.2. Selecao das Colunas\n",
    "        cols_drop = ['open', 'promo_interval', 'month_map']\n",
    "        df2 = df2.drop(cols_drop, axis=1)\n",
    "        \n",
    "        return df2\n",
    "    \n",
    "\n",
    "    def data_preparation(self, df5):\n",
    "        ## 5.2. Rescaling\n",
    "\n",
    "        # redimensionamento das variáveis numéricas para deixá-las na mesma escala\n",
    "        # distância até a concorrência\n",
    "        df5['competition_distance'] = self.competition_distance_scaler.fit_transform(df5[['competition_distance']].values)\n",
    "        \n",
    "        # tempo em meses desde a última competição\n",
    "        df5['competition_time_month'] = self.competition_time_month_scaler.fit_transform(df5[['competition_time_month']].values)\n",
    "       \n",
    "        # tempo em semanas desde a última promoção\n",
    "        df5['promo_time_week'] = self.promo_time_week_scaler.fit_transform(df5[['promo_time_week']].values)\n",
    "       \n",
    "        # ano\n",
    "        df5['year'] = self.year_scaler.fit_transform(df5[['year']].values)\n",
    "\n",
    "\n",
    "        ### 5.3.1. Encoding\n",
    "        # state_holiday - One Hot Encoding\n",
    "        # transformação de variáveis categóricas em variáveis binárias\n",
    "        df5 = pd.get_dummies(df5, prefix=['state_holiday'], columns=['state_holiday'])\n",
    "        \n",
    "        # store_type - Label Encoding\n",
    "        # transformação de variáveis categóricas em valores numéricos\n",
    "        df5['store_type'] = self.store_type_scaler.fit_transform(df5['store_type'])\n",
    "       \n",
    "        # assortment - Ordinal Encoding\n",
    "        # transformação de variáveis categóricas em valores ordinais\n",
    "        assortment_dict = {'basic': 1, 'extra': 2, 'extended': 3}\n",
    "        df5['assortment'] = df5['assortment'].map(assortment_dict)\n",
    "        \n",
    "        ### 5.3.3. Nature Transformation\n",
    "        # day of week\n",
    "        # transformação de variáveis de tempo em valores numéricos utilizando funções trigonométricas\n",
    "        df5['day_of_week_sin'] = df5['day_of_week'].apply(lambda x: np.sin(x * (2. * np.pi/7)))\n",
    "        df5['day_of_week_cos'] = df5['day_of_week'].apply(lambda x: np.cos(x * (2. * np.pi/7)))\n",
    "        \n",
    "        # month\n",
    "        df5['month_sin'] = df5['month'].apply(lambda x: np.sin(x * (2. * np.pi/12)))\n",
    "        df5['month_cos'] = df5['month'].apply(lambda x: np.cos(x * (2. * np.pi/12)))\n",
    "        \n",
    "        # day\n",
    "        df5['day_sin'] = df5['day'].apply(lambda x: np.sin(x * (2. * np.pi/30)))\n",
    "        df5['day_cos'] = df5['day'].apply(lambda x: np.cos(x * (2. * np.pi/30)))\n",
    "\n",
    "        # week of year\n",
    "        df5['week_of_year_sin'] = df5['week_of_year'].apply(lambda x: np.sin(x * (2. * np.pi/52)))\n",
    "        df5['week_of_year_cos'] = df5['week_of_year'].apply(lambda x: np.cos(x * (2. * np.pi/52)))\n",
    "\n",
    "        cols_selected = ['store', 'promo', 'store_type', 'assortment', 'competition_distance', 'competition_open_since_month',\n",
    "                         'competition_open_since_year', 'promo2', 'promo2_since_week', 'promo2_since_year', \n",
    "                         'competition_time_month', 'promo_time_week', 'day_of_week_sin', 'day_of_week_cos', 'month_sin', \n",
    "                         'month_cos', 'day_sin', 'day_cos', 'week_of_year_sin', 'week_of_year_cos']\n",
    "                 \n",
    "        return df5[cols_selected]\n",
    "\n",
    "\n",
    "def get_prediction(self, model, original_data, test_data):\n",
    "    \n",
    "    # prediction\n",
    "    pred = model.predict(test_data)\n",
    "    \n",
    "    # join pred into the original data\n",
    "    original_data['prediction'] = np.expm1(pred)\n",
    "    \n",
    "    return original_data.to_json(orient='records', date_format='iso')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.2 API Handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'rossmann'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mflask\u001b[39;00m \u001b[39mimport\u001b[39;00m Flask, request, Response\n\u001b[1;32m----> 4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mrossmann\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mRossmann\u001b[39;00m \u001b[39mimport\u001b[39;00m Rossmann\n\u001b[0;32m      7\u001b[0m \u001b[39m# Carregando o modelo salvo\u001b[39;00m\n\u001b[0;32m      8\u001b[0m model \u001b[39m=\u001b[39m pickle\u001b[39m.\u001b[39mload(\u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mC:\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mUsers\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mraquel\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mDocuments\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mComunidade DS\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mrepos\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39m05-DS-emProducao\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mrossmann_store_sales\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mmodel\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mmodel_rossmann.pkl\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m'\u001b[39m))\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'rossmann'"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "from flask import Flask, request, Response\n",
    "from rossmann.Rossmann import Rossmann\n",
    "\n",
    "\n",
    "# Carregando o modelo salvo\n",
    "model = pickle.load(open('C:\\\\Users\\\\raquel\\\\Documents\\\\Comunidade DS\\\\repos\\\\05-DS-emProducao\\\\rossmann_store_sales\\\\model\\\\model_rossmann.pkl', 'rb'))\n",
    "\n",
    "\n",
    "\n",
    "# Inicializando a API\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Definindo a rota e o método POST para a função de predição\n",
    "@app.route('/rossmann/predict', methods=['POST'])\n",
    "def rossmann_predict():\n",
    "    # Obtendo os dados em formato JSON\n",
    "    test_json = request.get_json()\n",
    "\n",
    "    # Verificando se há dados presentes\n",
    "    if test_json:\n",
    "        # Se houver apenas um exemplo\n",
    "        if isinstance(test_json, dict):\n",
    "            test_raw = pd.DataFrame(test_json, index=[0])\n",
    "        # Se houverem múltiplos exemplos\n",
    "        else:\n",
    "            test_raw = pd.DataFrame(test_json, columns=test_json[0].keys())\n",
    "\n",
    "        # Instanciando a classe Rossmann\n",
    "        pipeline = Rossmann()\n",
    "\n",
    "        # Realizando a limpeza dos dados\n",
    "        df1 = pipeline.data_cleaning(test_raw)\n",
    "\n",
    "        # Realizando a engenharia de features\n",
    "        df2 = pipeline.feature_engineering(df1)\n",
    "\n",
    "        # Preparando os dados\n",
    "        df3 = pipeline.data_preparation(df2)\n",
    "\n",
    "        # Realizando a predição\n",
    "        df_response = pipeline.get_prediction(model, test_raw, df3)\n",
    "\n",
    "        # Retornando os resultados em formato JSON\n",
    "        return df_response\n",
    "\n",
    "    else:\n",
    "        # Caso não hajam dados presentes, retornando um JSON vazio com código 200\n",
    "        return Response('{}', status=200, mimetype='application/json')\n",
    "\n",
    "# Iniciando a aplicação\n",
    "if __name__ == '__main__':\n",
    "    app.run('0.0.0.0')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.3 API Tester"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading test dataset\n",
    "df10 = pd.read_csv( 'C:\\\\Users\\\\raquel\\\\Documents\\\\Comunidade DS\\\\repos\\\\05-DS-emProducao\\\\rossmann_store_sales\\\\data\\\\test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge test dataset + store\n",
    "df_test = pd.merge( df10, df_store_raw, how='left', on='Store' )\n",
    "# choose store for prediction\n",
    "df_test = df_test[df_test['Store'].isin( [15, 24, 39] )]\n",
    "# remove closed days\n",
    "df_test = df_test[df_test['Open'] != 0]\n",
    "df_test = df_test[~df_test['Open'].isnull()]\n",
    "df_test = df_test.drop( 'Id', axis=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# convert Dataframe to json\n",
    "data = json.dumps( df_test.to_dict( orient='records' ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status Code 200\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# API Call\n",
    "url = 'https://rossmann-model-api.onrender.com/rossmann/predict'\n",
    "#url = 'http://127.0.0.1:5000/rossmann/predict'\n",
    "header = {'Content-type': 'application/json' }\n",
    "data = data\n",
    "r = requests.post(url, data=data, headers=header)\n",
    "print('Status Code {}'.format( r.status_code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = pd.DataFrame(r.json(), columns=r.json()[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Store Number 15 will sell R$303,752.68 in the next 6 weeks\n",
      "Store Number 24 will sell R$354,434.94 in the next 6 weeks\n",
      "Store Number 39 will sell R$155,208.82 in the next 6 weeks\n"
     ]
    }
   ],
   "source": [
    "d2 = d1[['store', 'prediction']].groupby( 'store' ).sum().reset_index()\n",
    "\n",
    "for i in range( len( d2 ) ):\n",
    "    print('Store Number {} will sell R${:,.2f} in the next 6 weeks'.format(\n",
    "        d2.loc[i, 'store'],\n",
    "        d2.loc[i, 'prediction']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds_em_producao",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
